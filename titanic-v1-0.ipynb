{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n# Import Data from CSV\ndf_test = pd.read_csv('../input/titanic/test.csv')\ndf_train = pd.read_csv('../input/titanic/train.csv')\n\n#Display Data Test\ndf_test.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-09T09:35:16.546395Z","iopub.execute_input":"2021-12-09T09:35:16.546874Z","iopub.status.idle":"2021-12-09T09:35:16.632039Z","shell.execute_reply.started":"2021-12-09T09:35:16.546768Z","shell.execute_reply":"2021-12-09T09:35:16.631184Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Display Data Train\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:16.634199Z","iopub.execute_input":"2021-12-09T09:35:16.634696Z","iopub.status.idle":"2021-12-09T09:35:16.652911Z","shell.execute_reply.started":"2021-12-09T09:35:16.634650Z","shell.execute_reply":"2021-12-09T09:35:16.652018Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Inspect Data Train\ndf_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:16.654508Z","iopub.execute_input":"2021-12-09T09:35:16.654877Z","iopub.status.idle":"2021-12-09T09:35:16.671274Z","shell.execute_reply.started":"2021-12-09T09:35:16.654840Z","shell.execute_reply":"2021-12-09T09:35:16.670363Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Clean Data","metadata":{}},{"cell_type":"code","source":"#Clean Data\n\n    #Count Rows\nindex = df_train.index\nnumber_of_rows = len(index)\nprint(number_of_rows)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:16.673686Z","iopub.execute_input":"2021-12-09T09:35:16.674310Z","iopub.status.idle":"2021-12-09T09:35:16.681688Z","shell.execute_reply.started":"2021-12-09T09:35:16.674268Z","shell.execute_reply":"2021-12-09T09:35:16.680640Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"    # Count total NaN at each column in a DataFrame\nprint(\" \\nCount total NaN at each column in a DataFrame : \\n\\n\",\n      df_train.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:16.683267Z","iopub.execute_input":"2021-12-09T09:35:16.683741Z","iopub.status.idle":"2021-12-09T09:35:16.699919Z","shell.execute_reply.started":"2021-12-09T09:35:16.683652Z","shell.execute_reply":"2021-12-09T09:35:16.698594Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"    #Drop 'Cabin' to many NaN's, Drop 'Ticket', 'Name' as not relevant\ndf_train = df_train.drop(columns=['Cabin', 'Name', 'Ticket'])\n\n    #Fill missing values in 'Age'\ndf_train['Age'] = df_train['Age'].interpolate()\n\n    #Fill mising Values in 'Embarked'\ndf_train['Embarked'] = df_train['Embarked'].interpolate()\n\n    # Count total NaN at each column in a DataFrame\nprint(\" \\nCount total NaN at each column in a DataFrame : \\n\\n\",\n      df_train.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:16.701417Z","iopub.execute_input":"2021-12-09T09:35:16.701777Z","iopub.status.idle":"2021-12-09T09:35:16.724783Z","shell.execute_reply.started":"2021-12-09T09:35:16.701743Z","shell.execute_reply":"2021-12-09T09:35:16.723824Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Get Corrolation of train data\ndf_train.corr()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:16.726053Z","iopub.execute_input":"2021-12-09T09:35:16.726343Z","iopub.status.idle":"2021-12-09T09:35:16.746049Z","shell.execute_reply.started":"2021-12-09T09:35:16.726311Z","shell.execute_reply":"2021-12-09T09:35:16.745344Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Create columns for Pclass, Sex, Embarked to allow analysis and modelling\n\n    #Copy df_train to df_train_2\ndf_train_2 = df_train.copy()\n\n    #Convert Pclass, Sex, Embarked to columns and attach to df_train_2\ndummies = []\ncols = ['Pclass','Sex','Embarked']\nfor col in cols:\n    dummies.append(pd.get_dummies(df_train_2[col]))\ntitanic_dummies = pd.concat(dummies, axis=1)\ndf_train_2 = pd.concat((df_train_2,titanic_dummies),axis=1)\ndf_train_2 = df_train_2.drop(['Pclass','Sex','Embarked'],axis=1)\ndf_train_2.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:16.747751Z","iopub.execute_input":"2021-12-09T09:35:16.747994Z","iopub.status.idle":"2021-12-09T09:35:16.780845Z","shell.execute_reply.started":"2021-12-09T09:35:16.747967Z","shell.execute_reply":"2021-12-09T09:35:16.780142Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Create ML model with sklearn","metadata":{}},{"cell_type":"code","source":"#Create ML\n\n    #Import sklearn\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n\n    #Create Numpy array\nX = df_train_2.values\ny = df_train_2['Survived'].values\n\n    #Drop 'Survived' from X\nX = np.delete(X,1,axis=1)\n\n    #Split data set to train and test 70/30 split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=0)\n\n    #Create Decision Tree\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(max_depth=5)\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:16.782041Z","iopub.execute_input":"2021-12-09T09:35:16.782406Z","iopub.status.idle":"2021-12-09T09:35:17.958964Z","shell.execute_reply.started":"2021-12-09T09:35:16.782371Z","shell.execute_reply":"2021-12-09T09:35:17.958307Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Run through four different model types to find best fit.","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\nclf = ensemble.RandomForestClassifier(n_estimators=100)\nclf.fit (X_train, y_train)\nclf.score (X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:17.961273Z","iopub.execute_input":"2021-12-09T09:35:17.961853Z","iopub.status.idle":"2021-12-09T09:35:18.348181Z","shell.execute_reply.started":"2021-12-09T09:35:17.961812Z","shell.execute_reply":"2021-12-09T09:35:18.347116Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"clf = ensemble.GradientBoostingClassifier()\nclf.fit (X_train, y_train)\nclf.score (X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:18.350228Z","iopub.execute_input":"2021-12-09T09:35:18.350553Z","iopub.status.idle":"2021-12-09T09:35:18.500210Z","shell.execute_reply.started":"2021-12-09T09:35:18.350511Z","shell.execute_reply":"2021-12-09T09:35:18.499209Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"clf = ensemble.GradientBoostingClassifier(n_estimators=50)\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:18.501592Z","iopub.execute_input":"2021-12-09T09:35:18.502276Z","iopub.status.idle":"2021-12-09T09:35:18.580203Z","shell.execute_reply.started":"2021-12-09T09:35:18.502230Z","shell.execute_reply":"2021-12-09T09:35:18.579578Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"With the best model fitted at 83% I'm pretty happy with that, so now using 'GradientBoostingClassifier' move on to import test data set","metadata":{}},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:18.581246Z","iopub.execute_input":"2021-12-09T09:35:18.581569Z","iopub.status.idle":"2021-12-09T09:35:18.596877Z","shell.execute_reply.started":"2021-12-09T09:35:18.581542Z","shell.execute_reply":"2021-12-09T09:35:18.596120Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Perfomr same data cleaning steps to the test set to match the train set.","metadata":{}},{"cell_type":"code","source":"    # Count total NaN at each column in a DataFrame\nprint(\" \\nCount total NaN at each column in a DataFrame : \\n\\n\",\n      df_test.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:18.598044Z","iopub.execute_input":"2021-12-09T09:35:18.598440Z","iopub.status.idle":"2021-12-09T09:35:18.611657Z","shell.execute_reply.started":"2021-12-09T09:35:18.598408Z","shell.execute_reply":"2021-12-09T09:35:18.610822Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"    #Drop 'Cabin' to many NaN's, Drop 'Ticket', 'Name' as not relevant\ndf_test = df_test.drop(columns=['Cabin', 'Name', 'Ticket'])\n\n    #Fill missing values in 'Age'\ndf_test['Age'] = df_test['Age'].interpolate()\n\n    #Fill missing values in 'Fare'\ndf_test['Fare'] = df_test['Fare'].interpolate()\n\n    # Count total NaN at each column in a DataFrame\nprint(\" \\nCount total NaN at each column in a DataFrame : \\n\\n\",\n      df_test.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:18.613239Z","iopub.execute_input":"2021-12-09T09:35:18.614155Z","iopub.status.idle":"2021-12-09T09:35:18.629589Z","shell.execute_reply.started":"2021-12-09T09:35:18.614110Z","shell.execute_reply":"2021-12-09T09:35:18.628654Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"    #Convert Pclass, Sex, Embarked to columns and attach to df_test\ndummies = []\ncols = ['Pclass','Sex','Embarked']\n    for col in cols:\n        dummies.append(pd.get_dummies(df_test[col]))\ntitanic_dummies = pd.concat(dummies, axis=1)\ndf_test = pd.concat((df_test,titanic_dummies),axis=1)\ndf_test = df_test.drop(['Pclass','Sex','Embarked'],axis=1)\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:18.631002Z","iopub.execute_input":"2021-12-09T09:35:18.631316Z","iopub.status.idle":"2021-12-09T09:35:18.658984Z","shell.execute_reply.started":"2021-12-09T09:35:18.631283Z","shell.execute_reply":"2021-12-09T09:35:18.658024Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Run the ML model on the test data.","metadata":{}},{"cell_type":"code","source":"#Run Model clf on Test Data\nX_results = df_test.values\ny_results = clf.predict(X_results)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:18.660912Z","iopub.execute_input":"2021-12-09T09:35:18.661452Z","iopub.status.idle":"2021-12-09T09:35:18.667030Z","shell.execute_reply.started":"2021-12-09T09:35:18.661407Z","shell.execute_reply":"2021-12-09T09:35:18.666095Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Output the results to CSV.","metadata":{}},{"cell_type":"code","source":"#Output to CSV for Kaggle Comp\noutput = np.column_stack((X_results[:,0],y_results))\ndf_results = pd.DataFrame(output.astype('int'),columns=['PassengerID','Survived'])\ndf_results.to_csv('titanic_results.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T09:35:18.668791Z","iopub.execute_input":"2021-12-09T09:35:18.669011Z","iopub.status.idle":"2021-12-09T09:35:18.684222Z","shell.execute_reply.started":"2021-12-09T09:35:18.668985Z","shell.execute_reply":"2021-12-09T09:35:18.683105Z"},"trusted":true},"execution_count":18,"outputs":[]}]}